{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arpWqzv-PvaX"
      },
      "outputs": [],
      "source": [
        "#!pip install datasets==3.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGeFLAyyz3VY"
      },
      "outputs": [],
      "source": [
        "# 1단계: Runtime → Factory reset runtime (또는 Disconnect and delete runtime)\n",
        "# 2단계: 새로 시작한 후\n",
        "#!pip install torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVa79AcPIGxf"
      },
      "outputs": [],
      "source": [
        "#@title FlashAttention GPT-3\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import math\n",
        "import time, gc\n",
        "import random\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tiktoken\n",
        "from collections import Counter\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from tqdm import tqdm\n",
        "from torch import amp\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model, num_heads, dropout, use_fused_qkv=True, block_size=128):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.d_k = d_model // num_heads\n",
        "    self.block_size = block_size\n",
        "\n",
        "    assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "    # fused QKV for efficient self-attention\n",
        "    # projects to 3 * d_model and we split into Q, K, V for the common self-attn path\n",
        "    self.use_fused_qkv = use_fused_qkv\n",
        "    self.w_qkv = nn.Linear(d_model, 3 * d_model) if use_fused_qkv else None\n",
        "    # keep separate projection linears available for cross-attention or clarity\n",
        "    self.w_q = nn.Linear(d_model, d_model)\n",
        "    self.w_k = nn.Linear(d_model, d_model)\n",
        "    self.w_v = nn.Linear(d_model, d_model)\n",
        "    self.w_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    # dropout applied to attention probabilities\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, q, k, v, tau=1.0, mask=None, block_sparse_mask=None):\n",
        "    \"\"\"\n",
        "    for standard self-attention q,k,v inputs are the same tensor.\n",
        "    We'll project q,k,v from their respective inputs but using the same fused layer\n",
        "    q: (b_q, q_len, d_model)\n",
        "    k: (b_k, kv_len, d_model)\n",
        "    v: (b_k, kv_len, d_model)\n",
        "    \"\"\"\n",
        "\n",
        "    b_q, q_len, _ = q.size()\n",
        "    b_k, kv_len, _ = k.size()\n",
        "\n",
        "    assert b_q == b_k, \"Batch sizes for q and k must match\"\n",
        "\n",
        "    # --- projections ---\n",
        "    # Fast path for self-attention when q,k,v are the same tensor and fused QKV is enabled\n",
        "    if self.use_fused_qkv and (q is k is v):\n",
        "        # self-attention common fast path: one linear -> split into Q,K,V\n",
        "        qkv = self.w_qkv(q)                       # (B, S_q, 3*d_model)\n",
        "        q_proj, k_proj, v_proj = qkv.chunk(3, dim=-1)\n",
        "    else:\n",
        "        # cross-attention or fused disabled: compute projections separately\n",
        "        if self.use_fused_qkv:\n",
        "          qkv = self.w_qkv(q)                     # (B, S_q, 3*d_model)\n",
        "          q_proj = qkv[..., :self.d_model]        # (B, S_q, d_model)\n",
        "        else:\n",
        "          q_proj = self.w_q(q)                    # (B, S_q, d_model)\n",
        "        k_proj = self.w_k(k)                      # (B, S_k, d_model)\n",
        "        v_proj = self.w_v(v)                      # (B, S_k, d_model)\n",
        "\n",
        "    # reshape to (B, H, S, d_k) for multi-head matmuls\n",
        "    q = q_proj.view(b_q, q_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "    k = k_proj.view(b_k, kv_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "    v = v_proj.view(b_k, kv_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    # Block-sparse FlashAttention or standard attention\n",
        "    if block_sparse_mask is not None:\n",
        "        out = self._block_sparse_flash_attention(q, k, v, tau, mask, block_sparse_mask)\n",
        "    else:\n",
        "        # scaled dot-product attention (dense)\n",
        "        scores = tau * torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            # mask: positions with 0 are set to -inf so softmax makes them zero\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "        out = torch.matmul(attn, v) # (B, H, S_q, d_k)\n",
        "\n",
        "    # combine heads and project output\n",
        "    out = out.transpose(1, 2).reshape(b_q, q_len, self.d_model)\n",
        "    out = self.w_o(out)\n",
        "    return out\n",
        "\n",
        "  def _block_sparse_flash_attention(self, q, k, v, tau=1.0, mask=None, block_sparse_mask=None):\n",
        "    \"\"\"Block-Sparse FlashAttention (Algorithm 5)\"\"\"\n",
        "    b, num_heads, q_len, d_k = q.shape\n",
        "    kv_len = k.shape[2]\n",
        "\n",
        "    # Block sizes (columns/rows processed per iteration)\n",
        "    Bc = min(self.block_size, kv_len)\n",
        "    Br = min(self.block_size, q_len)\n",
        "\n",
        "    # Initialize output accumulator O and softmax statistics l (denominator) and m (max logit)\n",
        "    O = torch.zeros_like(q)\n",
        "    l = torch.zeros(b, num_heads, q_len, 1, device=q.device)\n",
        "    m = torch.full((b, num_heads, q_len, 1), float('-inf'), device=q.device)\n",
        "\n",
        "    Tc = math.ceil(kv_len / Bc)\n",
        "    Tr = math.ceil(q_len / Br)\n",
        "\n",
        "    # Outer loop over K, V blocks\n",
        "    for j in range(Tc):\n",
        "        j_start = j * Bc\n",
        "        j_end = min((j + 1) * Bc, kv_len)\n",
        "\n",
        "        # Copy block-of-K and block-of-V\n",
        "        # Load Kj, Vj from HBM to SRAM\n",
        "        Kj = k[:, :, j_start:j_end, :]\n",
        "        Vj = v[:, :, j_start:j_end, :]\n",
        "\n",
        "        # Inner loop over Q blocks\n",
        "        for i in range(Tr):\n",
        "            # Skip zero blocks in block-sparse mask to save compute (Algorithm 5, line 8)\n",
        "            if block_sparse_mask[i, j] == 0:\n",
        "                continue\n",
        "\n",
        "            i_start = i * Br\n",
        "            i_end = min((i + 1) * Br, q_len)\n",
        "\n",
        "            # Load Qi, Oi, li, mi\n",
        "            Qi = q[:, :, i_start:i_end, :]\n",
        "            Oi = O[:, :, i_start:i_end, :]\n",
        "            li = l[:, :, i_start:i_end, :]\n",
        "            mi = m[:, :, i_start:i_end, :]\n",
        "\n",
        "            # Compute scaled dot-product Sij (Qi @ Kj^T)\n",
        "            Sij = tau * torch.matmul(Qi, Kj.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "            # Apply mask\n",
        "            if mask is not None:\n",
        "                mask_block = mask[:, :, i_start:i_end, j_start:j_end]\n",
        "                Sij = Sij.masked_fill(mask_block == 0, float('-inf'))\n",
        "\n",
        "            # Compute numerically-stable exponentials relative to block max\n",
        "            mij_tilde = torch.max(Sij, dim=-1, keepdim=True)[0]\n",
        "            Pij_tilde = torch.exp(Sij - mij_tilde)  # positive values (no dropout applied yet)\n",
        "\n",
        "            # --- sample dropout mask and rescale (match ForwardPass behavior) ---\n",
        "            p = float(self.dropout.p) if self.training else 0.0\n",
        "            if p > 0.0 and self.training:\n",
        "                rnd = torch.rand_like(Pij_tilde)\n",
        "                mask_ij = (rnd > p)\n",
        "                Pij_dropped = Pij_tilde * mask_ij.to(Pij_tilde.dtype) / (1.0 - p)\n",
        "            else:\n",
        "                mask_ij = None\n",
        "                Pij_dropped = Pij_tilde\n",
        "\n",
        "            # Use the dropped (and rescaled) values when computing sums/outputs\n",
        "            lij_tilde = torch.sum(Pij_dropped, dim=-1, keepdim=True)\n",
        "\n",
        "            # Update mi_new, li_new in a numerically stable way\n",
        "            mi_new = torch.max(mi, mij_tilde)\n",
        "            li_new = torch.exp(mi - mi_new) * li + torch.exp(mij_tilde - mi_new) * lij_tilde\n",
        "\n",
        "            # Update Oi using dropped Pij (consistent with forward)\n",
        "            Oi_new = (Oi * torch.exp(mi - mi_new) * li +\n",
        "                      torch.matmul(Pij_dropped, Vj) * torch.exp(mij_tilde - mi_new)) / li_new\n",
        "\n",
        "            # Write back updated accumulators and output block\n",
        "            O[:, :, i_start:i_end, :] = Oi_new\n",
        "            l[:, :, i_start:i_end, :] = li_new\n",
        "            m[:, :, i_start:i_end, :] = mi_new\n",
        "\n",
        "    return O\n",
        "\n",
        "def look_ahead_mask_(q_len, k_len=None, device=None):\n",
        "    \"\"\"\n",
        "    Improved causal mask:\n",
        "      - supports q_len != k_len (useful when using cached past key/values)\n",
        "      - returns a boolean mask of shape (1, 1, q_len, k_len) where True = allowed, False = masked\n",
        "    \"\"\"\n",
        "    if k_len is None:\n",
        "        k_len = q_len\n",
        "    device = device if device is not None else torch.device('cpu')\n",
        "\n",
        "    q_idx = torch.arange(q_len, device=device).unsqueeze(1)   # (q_len, 1)\n",
        "    k_idx = torch.arange(k_len, device=device).unsqueeze(0)   # (1, k_len)\n",
        "    offset = k_len - q_len\n",
        "    mask = (k_idx <= (q_idx + offset))                        # (q_len, k_len)\n",
        "    return mask.unsqueeze(0).unsqueeze(0)                     # (1, 1, q_len, k_len)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, d_model, num_heads, d_ff, dropout, use_fused_qkv=True, block_size=128):\n",
        "    super().__init__()\n",
        "\n",
        "    self.attn = MultiHeadAttention(d_model, num_heads, dropout, use_fused_qkv, block_size) #Masked MHA\n",
        "    self.dropout1 = nn.Dropout(dropout)\n",
        "    self.layer_norm1 = nn.LayerNorm(d_model)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, num_heads, dropout, block_size)\n",
        "    self.dropout2 = nn.Dropout(dropout)\n",
        "    self.layer_norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self, x, look_ahead_mask_=None, tau=1.0):\n",
        "    attention_out = self.attn(x, x, x, tau, look_ahead_mask_)\n",
        "    x = x + self.dropout1(attention_out)\n",
        "    x = self.layer_norm1(x)\n",
        "\n",
        "    ffn_out = self.ffn(x)\n",
        "    x = x + self.dropout2(ffn_out)\n",
        "    x = self.layer_norm2(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class DecoderStack(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, d_ff, dropout, use_fused_qkv, block_size=128):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            Decoder(d_model, num_heads, d_ff, dropout, use_fused_qkv, block_size)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, look_ahead_mask_=None, tau=1.0):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, look_ahead_mask_, tau)\n",
        "        return x\n",
        "\n",
        "class ForwardPass(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout, block_size=128):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def forward(self, q, k, v, tau=1.0, mask=None, block_sparse_mask=None):\n",
        "        \"\"\"\n",
        "        q, k, v: (B, N, d_model)\n",
        "        mask: (B, 1, N, N)\n",
        "        block_sparse_mask: (num_blocks_Q, num_blocks_K)\n",
        "        \"\"\"\n",
        "        B, N, _ = q.shape\n",
        "        H = self.num_heads\n",
        "        d_k = self.d_k\n",
        "\n",
        "        # reshape for multi-head\n",
        "        q = q.view(B, N, H, d_k).transpose(1, 2)  # (B, H, N, d_k)\n",
        "        k = k.view(B, N, H, d_k).transpose(1, 2)\n",
        "        v = v.view(B, N, H, d_k).transpose(1, 2)\n",
        "\n",
        "        # block sizes\n",
        "        Br = min(self.block_size or N, N)\n",
        "        Bc = min(self.block_size or N, N)\n",
        "\n",
        "        Tr = math.ceil(N / Br)\n",
        "        Tc = math.ceil(N / Bc)\n",
        "\n",
        "        # initialize outputs and softmax stats\n",
        "        O = torch.zeros_like(q)\n",
        "        l = torch.zeros(B, H, N, 1, device=q.device)\n",
        "        m = torch.full((B, H, N, 1), float('-inf'), device=q.device)\n",
        "\n",
        "        masks = []\n",
        "\n",
        "        for j in range(Tc):\n",
        "            j_start = j * Bc\n",
        "            j_end = min((j + 1) * Bc, N)\n",
        "            Kj = k[:, :, j_start:j_end, :]\n",
        "            Vj = v[:, :, j_start:j_end, :]\n",
        "\n",
        "            for i in range(Tr):\n",
        "                if block_sparse_mask is not None and block_sparse_mask[i, j] == 0:\n",
        "                    continue\n",
        "\n",
        "                i_start = i * Br\n",
        "                i_end = min((i + 1) * Br, N)\n",
        "\n",
        "                Qi = q[:, :, i_start:i_end, :]\n",
        "                Oi = O[:, :, i_start:i_end, :]\n",
        "                li = l[:, :, i_start:i_end, :]\n",
        "                mi = m[:, :, i_start:i_end, :]\n",
        "\n",
        "                # scaled dot-product\n",
        "                Sij = tau * torch.matmul(Qi, Kj.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "                if mask is not None:\n",
        "                    Sij = Sij.masked_fill(mask[:, :, i_start:i_end, j_start:j_end] == 0, float('-inf'))\n",
        "\n",
        "                # numerically stable exponent\n",
        "                mij_tilde = torch.max(Sij, dim=-1, keepdim=True)[0]\n",
        "                Pij_tilde = torch.exp(Sij - mij_tilde)  # positive values (no dropout applied yet)\n",
        "\n",
        "                # --- explicit block dropout mask (sample once and save) ---\n",
        "                p = float(self.dropout.p) if self.training else 0.0\n",
        "\n",
        "                if p > 0.0 and self.training:\n",
        "                    # sample binary mask with same shape as Pij_tilde, then rescale to keep expectation\n",
        "                    rnd = torch.rand_like(Pij_tilde)\n",
        "                    mask_ij = (rnd > p) # bool tensor\n",
        "                    Pij_dropped = Pij_tilde * mask_ij.to(Pij_tilde.dtype) / (1.0 - p)\n",
        "                else:\n",
        "                    mask_ij = None\n",
        "                    Pij_dropped = Pij_tilde\n",
        "\n",
        "                # save mask entry (keeps one-to-one ordering with block traversal)\n",
        "                # masks is a list declared earlier: masks = []\n",
        "                masks.append(mask_ij)\n",
        "\n",
        "                # IMPORTANT: use dropped version to compute sums (consistency)\n",
        "                lij_tilde = torch.sum(Pij_dropped, dim=-1, keepdim=True)\n",
        "\n",
        "                mi_new = torch.max(mi, mij_tilde)\n",
        "                li_new = torch.exp(mi - mi_new) * li + torch.exp(mij_tilde - mi_new) * lij_tilde\n",
        "\n",
        "                Oi_new = (Oi * torch.exp(mi - mi_new) * li +\n",
        "                          torch.matmul(Pij_dropped, Vj) * torch.exp(mij_tilde - mi_new)) / li_new\n",
        "\n",
        "                # write back\n",
        "                O[:, :, i_start:i_end, :] = Oi_new\n",
        "                l[:, :, i_start:i_end, :] = li_new\n",
        "                m[:, :, i_start:i_end, :] = mi_new\n",
        "\n",
        "        return O.transpose(1, 2).reshape(B, N, self.d_model), l, m, masks\n",
        "\n",
        "class BackwardPass(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout, block_size=128):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def forward(self, q, k, v, dO, tau=1.0, mask=None, block_sparse_mask=None, masks=None):\n",
        "        \"\"\"\n",
        "        q, k, v: (B, N, d_model)\n",
        "        dO: gradient of output (B, N, d_model)\n",
        "        mask: (B, 1, N, N)\n",
        "        block_sparse_mask: (num_blocks_Q, num_blocks_K)\n",
        "        Returns:\n",
        "            dQ, dK, dV: gradients w.r.t Q, K, V\n",
        "        \"\"\"\n",
        "        B, N, _ = q.shape\n",
        "        H = self.num_heads\n",
        "        d_k = self.d_k\n",
        "\n",
        "        # reshape for multi-head\n",
        "        q = q.view(B, N, H, d_k).transpose(1, 2)  # (B, H, N, d_k)\n",
        "        k = k.view(B, N, H, d_k).transpose(1, 2)\n",
        "        v = v.view(B, N, H, d_k).transpose(1, 2)\n",
        "        dO = dO.view(B, N, H, d_k).transpose(1, 2)\n",
        "\n",
        "        Br = min(self.block_size or N, N)\n",
        "        Bc = min(self.block_size or N, N)\n",
        "        Tr = math.ceil(N / Br)\n",
        "        Tc = math.ceil(N / Bc)\n",
        "\n",
        "        # initialize gradients\n",
        "        dQ = torch.zeros_like(q)\n",
        "        dK = torch.zeros_like(k)\n",
        "        dV = torch.zeros_like(v)\n",
        "\n",
        "        # softmax statistics placeholders\n",
        "        l = torch.zeros(B, H, N, 1, device=q.device)\n",
        "        m = torch.full((B, H, N, 1), float('-inf'), device=q.device)\n",
        "\n",
        "        mask_idx = 0\n",
        "\n",
        "        for j in range(Tc):\n",
        "          j_start = j * Bc\n",
        "          j_end = min((j + 1) * Bc, N)\n",
        "          Kj = k[:, :, j_start:j_end, :]\n",
        "          Vj = v[:, :, j_start:j_end, :]\n",
        "\n",
        "          # accumulate gradients for this Kj,Vj across i's, then write once after the i-loop\n",
        "          dKj = torch.zeros_like(Kj)\n",
        "          dVj = torch.zeros_like(Vj)\n",
        "\n",
        "          for i in range(Tr):\n",
        "              if block_sparse_mask is not None and block_sparse_mask[i, j] == 0:\n",
        "                  continue\n",
        "\n",
        "              i_start = i * Br\n",
        "              i_end = min((i + 1) * Br, N)\n",
        "\n",
        "              Qi = q[:, :, i_start:i_end, :]\n",
        "              dOi = dO[:, :, i_start:i_end, :]\n",
        "              li = l[:, :, i_start:i_end, :]\n",
        "              mi = m[:, :, i_start:i_end, :]\n",
        "\n",
        "              # scaled dot-product (recompute)\n",
        "              Sij = tau * torch.matmul(Qi, Kj.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "              if mask is not None:\n",
        "                  Sij = Sij.masked_fill(mask[:, :, i_start:i_end, j_start:j_end] == 0, float('-inf'))\n",
        "\n",
        "              mij_tilde = torch.max(Sij, dim=-1, keepdim=True)[0]\n",
        "              Pij = torch.exp(Sij - mij_tilde)\n",
        "              # retrieve the same dropout mask used in forward\n",
        "              mask_ij = None\n",
        "              if masks is not None:\n",
        "                  mask_ij = masks[mask_idx]  # masks defined/passed in outer scope\n",
        "                  mask_idx += 1\n",
        "\n",
        "              # if mask present, apply same binary mask and rescale\n",
        "              p = float(self.dropout.p) if self.training else 0.0\n",
        "\n",
        "              if mask_ij is not None:\n",
        "                  Pij = Pij * mask_ij / (1.0 - p)\n",
        "\n",
        "              # normalized attention (A)\n",
        "              lij = torch.sum(Pij, dim=-1, keepdim=True)\n",
        "              Pij_norm = Pij / (lij + 1e-12)  # avoid divide-by-zero\n",
        "\n",
        "              # backward: dA = dOi @ Vj^T\n",
        "              dA = torch.matmul(dOi, Vj.transpose(-2, -1))\n",
        "\n",
        "              # softmax jacobian: dS = A * (dA - sum(dA * A, dim=-1, keepdim=True))\n",
        "              tmp = torch.sum(dA * Pij_norm, dim=-1, keepdim=True)\n",
        "              dS = Pij_norm * (dA - tmp)\n",
        "\n",
        "              # accumulate gradient w.r.t Vj across i\n",
        "              dVj += torch.matmul(Pij_norm.transpose(-2, -1), dOi)\n",
        "\n",
        "              # include tau scaling consistently (forward used tau / sqrt(d_k))\n",
        "              scale = tau / math.sqrt(d_k)\n",
        "              dQi = torch.matmul(dS, Kj) * scale\n",
        "              dKj += torch.matmul(dS.transpose(-2, -1), Qi) * scale\n",
        "\n",
        "              # accumulate gradient for Q immediately (per-i block)\n",
        "              dQ[:, :, i_start:i_end, :] += dQi\n",
        "\n",
        "          # after finishing all i for this j, write the accumulated dKj,dVj once\n",
        "          dK[:, :, j_start:j_end, :] += dKj\n",
        "          dV[:, :, j_start:j_end, :] += dVj\n",
        "\n",
        "\n",
        "        # reshape back to (B, N, d_model)\n",
        "        dQ = dQ.transpose(1, 2).reshape(B, N, self.d_model)\n",
        "        dK = dK.transpose(1, 2).reshape(B, N, self.d_model)\n",
        "        dV = dV.transpose(1, 2).reshape(B, N, self.d_model)\n",
        "\n",
        "        return dQ, dK, dV\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    FeedForward that fuses projections + block-wise FlashAttention forward/backward\n",
        "    using the provided ForwardPass and BackwardPass modules.\n",
        "\n",
        "    Usage: replace the old FeedForward class with this. It expects self.w_q, w_k, w_v, w_o\n",
        "    to be nn.Linear modules (bias may be None).\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, dropout, block_size=128):\n",
        "        super().__init__()\n",
        "        # Projection & output layers\n",
        "        self.w_q = nn.Linear(d_model, d_model, bias=True)\n",
        "        self.w_k = nn.Linear(d_model, d_model, bias=True)\n",
        "        self.w_v = nn.Linear(d_model, d_model, bias=True)\n",
        "        self.w_o = nn.Linear(d_model, d_model, bias=True)\n",
        "\n",
        "        # low-level optimized forward/backward implementations\n",
        "        self.forward_pass = ForwardPass(d_model, num_heads, dropout, block_size)\n",
        "        self.backward_pass = BackwardPass(d_model, num_heads, dropout, block_size)\n",
        "\n",
        "    def forward(self, x, mask=None, block_sparse_mask=None, tau=1.0):\n",
        "        \"\"\"\n",
        "        x: (B, N, d_model)\n",
        "        mask: (B, 1, N, N) or None\n",
        "        block_sparse_mask: (num_blocks_Q, num_blocks_K) or None\n",
        "\n",
        "        This uses a custom autograd.Function to ensure the backward uses the\n",
        "        optimized BackwardPass implementation and computes weight gradients\n",
        "        for the projection / output linears manually.\n",
        "        \"\"\"\n",
        "        # local alias to make call-sites shorter\n",
        "        return _FlashAttnFn.apply(\n",
        "            x,\n",
        "            self.w_q.weight, self.w_q.bias,\n",
        "            self.w_k.weight, self.w_k.bias,\n",
        "            self.w_v.weight, self.w_v.bias,\n",
        "            self.w_o.weight, self.w_o.bias,\n",
        "            self.forward_pass, self.backward_pass,\n",
        "            tau, mask, block_sparse_mask\n",
        "        )\n",
        "\n",
        "\n",
        "class _FlashAttnFn(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Custom autograd Function:\n",
        "      forward:  x -> (q,k,v) via linear weights -> ForwardPass -> out @ W_o^T + b_o\n",
        "      backward: compute d_out -> d_output_attn -> use BackwardPass to get dQ,dK,dV\n",
        "                then compute gradients for weights/biases and input x analytically.\n",
        "    Note: mask and block_sparse_mask are treated as non-differentiable inputs (grad None).\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, x,\n",
        "                wq_w, wq_b, wk_w, wk_b, wv_w, wv_b, wo_w, wo_b,\n",
        "                forward_pass, backward_pass,\n",
        "                tau, mask, block_sparse_mask):\n",
        "        # x: (B, N, d_model)\n",
        "        # Linear projections (manually, to be able to return grads for weights)\n",
        "        q = x.matmul(wq_w.t())\n",
        "        if wq_b is not None:\n",
        "            q = q + wq_b.view(1, 1, -1)\n",
        "\n",
        "        k = x.matmul(wk_w.t())\n",
        "        if wk_b is not None:\n",
        "            k = k + wk_b.view(1, 1, -1)\n",
        "\n",
        "        v = x.matmul(wv_w.t())\n",
        "        if wv_b is not None:\n",
        "            v = v + wv_b.view(1, 1, -1)\n",
        "\n",
        "        # call optimized forward pass (returns output, l, m)\n",
        "        output_attn, l, m, masks = forward_pass(q, k, v, tau, mask, block_sparse_mask)  # (B, N, d_model), l,m shapes\n",
        "        ctx.masks = masks\n",
        "\n",
        "        # final linear output projection\n",
        "        out = output_attn.matmul(wo_w.t())\n",
        "        if wo_b is not None:\n",
        "            out = out + wo_b.view(1, 1, -1)\n",
        "\n",
        "        # save for backward\n",
        "        ctx.save_for_backward(x, q, k, v, output_attn, wq_w, wk_w, wv_w, wo_w)\n",
        "        # store non-tensor objects / flags on ctx\n",
        "        ctx.wq_b_exists = (wq_b is not None)\n",
        "        ctx.wk_b_exists = (wk_b is not None)\n",
        "        ctx.wv_b_exists = (wv_b is not None)\n",
        "        ctx.wo_b_exists = (wo_b is not None)\n",
        "        ctx.wq_b = wq_b\n",
        "        ctx.wk_b = wk_b\n",
        "        ctx.wv_b = wv_b\n",
        "        ctx.wo_b = wo_b\n",
        "\n",
        "        ctx.forward_pass = forward_pass\n",
        "        ctx.backward_pass = backward_pass\n",
        "        ctx.tau = float(tau)\n",
        "        # mask / block_sparse_mask may be tensors; we keep references but treat as non-differentiable\n",
        "        ctx.mask = mask\n",
        "        ctx.block_sparse_mask = block_sparse_mask\n",
        "\n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_out):\n",
        "        \"\"\"\n",
        "        Return gradients for:\n",
        "        (x,\n",
        "         wq_w, wq_b, wk_w, wk_b, wv_w, wv_b, wo_w, wo_b,\n",
        "         forward_pass, backward_pass, tau, mask, block_sparse_mask)\n",
        "        Non-tensor args -> return None placeholders.\n",
        "        \"\"\"\n",
        "        # restore\n",
        "        x, q, k, v, output_attn, wq_w, wk_w, wv_w, wo_w = ctx.saved_tensors\n",
        "        forward_pass = ctx.forward_pass\n",
        "        backward_pass = ctx.backward_pass\n",
        "        tau = ctx.tau\n",
        "        mask = ctx.mask\n",
        "        block_sparse_mask = ctx.block_sparse_mask\n",
        "\n",
        "        B, N, d_model = x.shape\n",
        "\n",
        "        # 1) grads through output linear (w_o)\n",
        "        # grad_out: (B, N, d_model)\n",
        "        # dW_o = grad_out_flat^T @ output_attn_flat\n",
        "        go_flat = grad_out.reshape(-1, d_model)\n",
        "        out_flat = output_attn.reshape(-1, d_model)\n",
        "        dWo = go_flat.t().matmul(out_flat)  # (d_model, d_model)\n",
        "        dbo = go_flat.sum(dim=0) if ctx.wo_b_exists else None\n",
        "\n",
        "        # d_output_attn = grad_out @ W_o\n",
        "        d_output_attn = grad_out.matmul(wo_w)  # (B, N, d_model)\n",
        "\n",
        "        # 2) call optimized backward to compute dQ, dK, dV\n",
        "        # BackwardPass expects shapes (B, N, d_model) for q,k,v and dO (grad of output_attn)\n",
        "        masks = getattr(ctx, \"masks\", None)\n",
        "        dQ, dK, dV = backward_pass(q, k, v, d_output_attn, tau, mask, block_sparse_mask, masks)\n",
        "        # dQ/dK/dV are (B, N, d_model)\n",
        "\n",
        "        # 3) compute gradients for projection weights and for input x:\n",
        "        x_flat = x.reshape(-1, d_model)                     # (B*N, d_model)\n",
        "        dQ_flat = dQ.reshape(-1, d_model)\n",
        "        dK_flat = dK.reshape(-1, d_model)\n",
        "        dV_flat = dV.reshape(-1, d_model)\n",
        "\n",
        "        # weight grads: dW = dProj^T @ x_flat\n",
        "        dWq = dQ_flat.t().matmul(x_flat)\n",
        "        dWk = dK_flat.t().matmul(x_flat)\n",
        "        dWv = dV_flat.t().matmul(x_flat)\n",
        "\n",
        "        # bias grads if present\n",
        "        dbq = dQ_flat.sum(dim=0) if ctx.wq_b_exists else None\n",
        "        dbk = dK_flat.sum(dim=0) if ctx.wk_b_exists else None\n",
        "        dbv = dV_flat.sum(dim=0) if ctx.wv_b_exists else None\n",
        "\n",
        "        # grads w.r.t input x from each projection: dx = dProj @ W\n",
        "        dx_q = dQ_flat.matmul(wq_w).reshape(B, N, d_model)\n",
        "        dx_k = dK_flat.matmul(wk_w).reshape(B, N, d_model)\n",
        "        dx_v = dV_flat.matmul(wv_w).reshape(B, N, d_model)\n",
        "\n",
        "        dx = dx_q + dx_k + dx_v  # total input gradient\n",
        "\n",
        "        # return gradient tuple matching forward signature\n",
        "        # (x, wq_w, wq_b, wk_w, wk_b, wv_w, wv_b, wo_w, wo_b, forward_pass, backward_pass, tau, mask, block_sparse_mask)\n",
        "        return (\n",
        "            dx,\n",
        "            dWq, dbq,\n",
        "            dWk, dbk,\n",
        "            dWv, dbv,\n",
        "            dWo, dbo,\n",
        "            None,  # forward_pass (non-tensor)\n",
        "            None,  # backward_pass (non-tensor)\n",
        "            None,  # tau (non-tensor / float)\n",
        "            None,  # mask (non-differentiable here)\n",
        "            None   # block_sparse_mask (non-differentiable here)\n",
        "        )\n",
        "\n",
        "\n",
        "class Embedding(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.emb = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.emb(x) * math.sqrt(self.d_model)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model, max_len, dropout):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.max_len = max_len\n",
        "\n",
        "    # learned positional embeddings\n",
        "    self.pos_emb = nn.Embedding(self.max_len, d_model)\n",
        "    # initialize similar to transformer practice\n",
        "    nn.init.normal_(self.pos_emb.weight, mean=0.0, std=0.02)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x: (B, L, d_model)\n",
        "    b, l, _ = x.size()\n",
        "    positions = torch.arange(l, device=x.device).unsqueeze(0)  # (1, L)\n",
        "    pos = self.pos_emb(positions)                             # (1, L, d_model)\n",
        "    x = x + pos\n",
        "    return self.dropout(x)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, dropout, max_len, use_fused_qkv=True, block_size=128):\n",
        "        super().__init__()\n",
        "\n",
        "        # single token embedding (use this for input tokens)\n",
        "        self.token_embedding = Embedding(vocab_size, d_model)\n",
        "        # learned positional encoding\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout)\n",
        "\n",
        "        # decoder-only stack\n",
        "        self.decoder = nn.ModuleList([\n",
        "            Decoder(d_model, num_heads, d_ff, dropout, use_fused_qkv, block_size)  # Pass d_ff to each Decoder\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # language modeling head\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, input_ids, tgt_mask=None, tau=1.0):\n",
        "        x = self.token_embedding(input_ids)       # (B, L, d_model)\n",
        "        x = self.pos_encoding(x)                  # (B, L, d_model)\n",
        "\n",
        "        for layer in self.decoder:\n",
        "            x = layer(x, tgt_mask, tau)\n",
        "\n",
        "        logits = self.fc_out(x)                  # (B, L, vocab_size)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xf9GUUzVPs46"
      },
      "outputs": [],
      "source": [
        "#@title GPT-3 Config Setup\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class GPT3Config:\n",
        "    def __init__(self):\n",
        "        # Model architecture\n",
        "        self.vocab_size = 50257     # Size of the GPT-3 tokenizer vocabulary\n",
        "        self.d_model = 768          # Model hidden dimension (GPT-3 175B uses 12288)\n",
        "        self.n_layers = 12          # Number of Transformer decoder layers (GPT-3 175B uses 96 layers)\n",
        "        self.n_heads = 12           # Number of attention heads per layer (GPT-3 175B uses 96 heads)\n",
        "        self.d_ff = 3072            # Feed-forward network hidden dimension (GPT-3 175B uses 49152)\n",
        "        self.dropout = 0.1          # Dropout rate (GPT-3 paper did not use dropout)\n",
        "        self.max_seq_len = 512      # Maximum sequence length (GPT-3 uses up to 2048 tokens)\n",
        "\n",
        "        # Optimization / hyperparameters\n",
        "        self.lr = 1e-4              # Learning rate for Adam optimizer\n",
        "        self.betas = (0.9, 0.95)    # Beta values for Adam optimizer\n",
        "        self.eps = 1e-8             # Epsilon for numerical stability in Adam optimizer\n",
        "        self.weight_decay = 0.0     # Weight decay for regularization\n",
        "        self.warmup_steps = 1000    # Number of steps to linearly warm up the LR\n",
        "        self.lr_decay = \"cosine\"    # Learning rate decay schedule after warmup\n",
        "\n",
        "        # FlashAttention / blocking\n",
        "        self.block_size = 128             # Block size used by block-sparse\n",
        "        self.use_flash_attention = True   # Flag to enable FlashAttention\n",
        "\n",
        "        # Model details\n",
        "        self.activation = \"gelu\"          # Activation function used in feed-forward layers\n",
        "        self.initializer_range = 0.02     # Stddev for weight initialization\n",
        "\n",
        "        # A100 optimization setup\n",
        "        self.gradient_accumulation_steps = 16  # Large batch simulation\n",
        "        self.mixed_precision = True            # FP16/BF16\n",
        "        self.compile_model = True              # torch.compile\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Automatically select GPU if available, else use CPU\n",
        "        self.epochs = 5                        # Number of epochs to train\n",
        "\n",
        "config = GPT3Config()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpC5CZK_QDf1",
        "outputId": "3d5dc7a0-7a9e-428c-aa99-932a1536972a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total raw texts loaded: 400,688\n",
            "Tokenizer vocab size: 100,277\n",
            "Updated config vocab_size to: 100,277\n",
            "Tokenizing dataset...\n",
            "Progress: 10,000/400,688 (2.5%) - Elapsed: 0.1min\n",
            "Progress: 20,000/400,688 (5.0%) - Elapsed: 0.2min\n",
            "Progress: 30,000/400,688 (7.5%) - Elapsed: 0.4min\n",
            "Progress: 40,000/400,688 (10.0%) - Elapsed: 0.5min\n",
            "Progress: 50,000/400,688 (12.5%) - Elapsed: 0.6min\n",
            "Progress: 60,000/400,688 (15.0%) - Elapsed: 0.7min\n",
            "Progress: 70,000/400,688 (17.5%) - Elapsed: 0.8min\n",
            "Progress: 80,000/400,688 (20.0%) - Elapsed: 0.9min\n",
            "Progress: 90,000/400,688 (22.5%) - Elapsed: 1.0min\n",
            "Progress: 100,000/400,688 (25.0%) - Elapsed: 1.1min\n",
            "Progress: 110,000/400,688 (27.5%) - Elapsed: 1.3min\n",
            "Progress: 120,000/400,688 (29.9%) - Elapsed: 1.4min\n",
            "Progress: 130,000/400,688 (32.4%) - Elapsed: 1.5min\n",
            "Progress: 140,000/400,688 (34.9%) - Elapsed: 1.6min\n",
            "Progress: 150,000/400,688 (37.4%) - Elapsed: 1.7min\n",
            "Progress: 160,000/400,688 (39.9%) - Elapsed: 1.8min\n",
            "Progress: 170,000/400,688 (42.4%) - Elapsed: 1.9min\n",
            "Progress: 180,000/400,688 (44.9%) - Elapsed: 2.0min\n",
            "Progress: 190,000/400,688 (47.4%) - Elapsed: 2.2min\n",
            "Progress: 200,000/400,688 (49.9%) - Elapsed: 2.3min\n",
            "Progress: 210,000/400,688 (52.4%) - Elapsed: 2.4min\n",
            "Progress: 220,000/400,688 (54.9%) - Elapsed: 2.5min\n",
            "Progress: 230,000/400,688 (57.4%) - Elapsed: 2.6min\n",
            "Progress: 240,000/400,688 (59.9%) - Elapsed: 2.7min\n",
            "Progress: 250,000/400,688 (62.4%) - Elapsed: 2.8min\n",
            "Progress: 260,000/400,688 (64.9%) - Elapsed: 2.9min\n",
            "Progress: 270,000/400,688 (67.4%) - Elapsed: 3.0min\n",
            "Progress: 280,000/400,688 (69.9%) - Elapsed: 3.1min\n",
            "Progress: 290,000/400,688 (72.4%) - Elapsed: 3.3min\n",
            "Progress: 300,000/400,688 (74.9%) - Elapsed: 3.4min\n",
            "Progress: 310,000/400,688 (77.4%) - Elapsed: 3.5min\n",
            "Progress: 320,000/400,688 (79.9%) - Elapsed: 3.6min\n",
            "Progress: 330,000/400,688 (82.4%) - Elapsed: 3.7min\n",
            "Progress: 340,000/400,688 (84.9%) - Elapsed: 3.8min\n",
            "Progress: 350,000/400,688 (87.3%) - Elapsed: 3.9min\n",
            "Progress: 360,000/400,688 (89.8%) - Elapsed: 4.0min\n",
            "Progress: 370,000/400,688 (92.3%) - Elapsed: 4.1min\n",
            "Progress: 380,000/400,688 (94.8%) - Elapsed: 4.3min\n",
            "Progress: 390,000/400,688 (97.3%) - Elapsed: 4.4min\n",
            "Progress: 400,000/400,688 (99.8%) - Elapsed: 4.5min\n",
            "Tokenization completed in 4.5 minutes\n",
            "Total tokens: 429,532,955\n",
            "Total usable sequences: 1,677,863\n",
            "Sample input shape: torch.Size([255]), dtype=torch.int64\n",
            "Sample target shape: torch.Size([255]), dtype=torch.int64\n"
          ]
        }
      ],
      "source": [
        "#@title GPT-3 Dataset Preparation\n",
        "\n",
        "class GPT3Dataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_seq_len, vocab_size=None, sequential=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            texts (list[str]): Raw text samples\n",
        "            tokenizer: GPT tokenizer (e.g., tiktoken)\n",
        "            max_seq_len (int): Max sequence length\n",
        "            vocab_size (int, optional): Limit vocab to config size\n",
        "            sequential (bool):\n",
        "                - True: sequential slicing of tokens (deterministic)\n",
        "                - False: random subsequence sampling (better generalization)\n",
        "        \"\"\"\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.vocab_size = vocab_size\n",
        "        self.sequential = sequential\n",
        "\n",
        "        # Tokenize all texts once into one long stream\n",
        "        print(\"Tokenizing dataset...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        self.tokens = []\n",
        "        for i, text in enumerate(texts):\n",
        "            # encode returns list[int] token ids\n",
        "            token_ids = tokenizer.encode(text)\n",
        "            self.tokens.extend(token_ids)\n",
        "\n",
        "            # Progress logging every 10k texts helps estimate runtime on large corpora\n",
        "            if (i + 1) % 10000 == 0:\n",
        "                elapsed = (time.time() - start_time) / 60\n",
        "                progress = (i + 1) / len(texts) * 100\n",
        "                print(f\"Progress: {i+1:,}/{len(texts):,} ({progress:.1f}%) - Elapsed: {elapsed:.1f}min\")\n",
        "\n",
        "        # Tokenization completed\n",
        "        total_time = (time.time() - start_time) / 60\n",
        "        print(f\"Tokenization completed in {total_time:.1f} minutes\")\n",
        "\n",
        "        # store stats used by __len__ and __getitem__\n",
        "        self.total_tokens = len(self.tokens)\n",
        "        # Number of full sequences\n",
        "        self.num_sequences = self.total_tokens // self.max_seq_len\n",
        "        print(f\"Total tokens: {self.total_tokens:,}\")\n",
        "        print(f\"Total usable sequences: {self.num_sequences:,}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_sequences\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Determine start index for the requested sequence\n",
        "        if self.sequential:\n",
        "            start = idx * self.max_seq_len\n",
        "        else:\n",
        "            # random offset for more variety\n",
        "            start = random.randint(0, self.total_tokens - self.max_seq_len - 1)\n",
        "\n",
        "        end = start + self.max_seq_len\n",
        "        seq = self.tokens[start:end]\n",
        "\n",
        "        # Ensure fixed length by padding with 0's\n",
        "        if len(seq) < self.max_seq_len:\n",
        "            seq += [0] * (self.max_seq_len - len(seq))\n",
        "\n",
        "        # Clamp IDs to vocab_size\n",
        "        seq = [min(t, self.vocab_size - 1) for t in seq]\n",
        "\n",
        "        # Inputs are tokens[:-1], targets are tokens[1:] (next-token prediction)\n",
        "        input_ids = torch.tensor(seq[:-1], dtype=torch.long)\n",
        "        target_ids = torch.tensor(seq[1:], dtype=torch.long)\n",
        "        return input_ids, target_ids\n",
        "\n",
        "\n",
        "# --- Load dataset (OpenWebText) ---\n",
        "dataset_owt = load_dataset(\"openwebtext\", split=\"train[:5%]\")\n",
        "texts = dataset_owt['text'][:1_500_000]  # sampling (≈300M target tokens)\n",
        "\n",
        "print(f\"Total raw texts loaded: {len(texts):,}\")\n",
        "\n",
        "# --- GPT-3 tokenizer ---\n",
        "tokenizer = tiktoken.get_encoding(\"cl100k_base\")  # GPT-3 BPE tokenizer\n",
        "print(f\"Tokenizer vocab size: {tokenizer.n_vocab:,}\")\n",
        "\n",
        "# Update config vocab_size\n",
        "config.vocab_size = tokenizer.n_vocab\n",
        "print(f\"Updated config vocab_size to: {config.vocab_size:,}\")\n",
        "\n",
        "# --- Dataset & DataLoader ---\n",
        "max_seq_len = 256  # adjust based on GPU memory\n",
        "train_dataset = GPT3Dataset(\n",
        "    texts, tokenizer, max_seq_len=max_seq_len,  # keep tokenizer's vocab by default\n",
        "    vocab_size=config.vocab_size, sequential=False  # False → random subsequences\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True,\n",
        "    prefetch_factor=2\n",
        ")\n",
        "\n",
        "# Debug check\n",
        "sample_in, sample_out = train_dataset[0]\n",
        "print(f\"Sample input shape: {sample_in.shape}, dtype={sample_in.dtype}\")\n",
        "print(f\"Sample target shape: {sample_out.shape}, dtype={sample_out.dtype}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCGzmhoAQGPm"
      },
      "outputs": [],
      "source": [
        "#@title GPT-3 Training\n",
        "\n",
        "# Enables cuDNN autotuner to find the best algorithm for the hardware (improves training speed)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# Initialize Transformer model\n",
        "model = Transformer(\n",
        "    vocab_size=config.vocab_size,\n",
        "    d_model=config.d_model,\n",
        "    num_heads=config.n_heads,\n",
        "    num_layers=config.n_layers,\n",
        "    d_ff=config.d_ff,\n",
        "    dropout=config.dropout,\n",
        "    max_len=max_seq_len\n",
        ").to(config.device)\n",
        "\n",
        "# Compile model if requested\n",
        "if getattr(config, \"compile_model\", False):\n",
        "    try:\n",
        "        model = torch.compile(model, backend=\"inductor\")\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] torch.compile failed or incompatible:\", e)\n",
        "\n",
        "# Mixed precision scaler\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=bool(getattr(config, \"mixed_precision\", True)))\n",
        "\n",
        "# Optimizer & Scheduler\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=config.lr,\n",
        "    betas=config.betas,\n",
        "    eps=config.eps,\n",
        "    weight_decay=config.weight_decay\n",
        ")\n",
        "\n",
        "# Gradient accumulation & steps\n",
        "grad_accum_steps = max(1, getattr(config, \"gradient_accumulation_steps\", 1))\n",
        "steps_per_epoch = len(train_loader) // grad_accum_steps\n",
        "# Total number of optimization steps across all epochs\n",
        "total_steps = steps_per_epoch * config.epochs if steps_per_epoch > 0 else len(train_loader) * config.epochs\n",
        "\n",
        "# CosineAnnealingLR learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max(1, total_steps))\n",
        "\n",
        "# Stats\n",
        "flash_stats = {'time_per_batch': [], 'peak_memory_mb': []}\n",
        "log_interval = 20\n",
        "\n",
        "# --- Training loop ---\n",
        "model.train()\n",
        "for epoch in range(config.epochs):\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.reset_peak_memory_stats(device=config.device)\n",
        "\n",
        "    total_loss = 0.0\n",
        "    optimizer.zero_grad(set_to_none=True)   # Reset gradients efficiently\n",
        "\n",
        "    pbar = tqdm(enumerate(train_loader), total=len(train_loader),\n",
        "                desc=f\"Epoch {epoch+1}/{config.epochs}\", leave=True)\n",
        "\n",
        "    for batch_idx, (input_ids, target_ids) in pbar:\n",
        "        # move to GPU efficiently\n",
        "        input_ids = input_ids.to(config.device, non_blocking=True)\n",
        "        target_ids = target_ids.to(config.device, non_blocking=True)\n",
        "\n",
        "        # timing\n",
        "        if torch.cuda.is_available():\n",
        "            start_evt = torch.cuda.Event(enable_timing=True)\n",
        "            end_evt = torch.cuda.Event(enable_timing=True)\n",
        "            start_evt.record()\n",
        "        else:\n",
        "            start_time = time.time()\n",
        "\n",
        "        # forward + mixed precision\n",
        "        with torch.cuda.amp.autocast(enabled=config.mixed_precision):\n",
        "            logits = model(input_ids) # Forward pass\n",
        "            loss = criterion(logits.view(-1, config.vocab_size), target_ids.view(-1))\n",
        "            loss = loss / grad_accum_steps  # Scale loss for gradient accumulation\n",
        "\n",
        "        # backward with scaling\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # optimizer step with gradient accumulation\n",
        "        if ((batch_idx + 1) % grad_accum_steps == 0) or (batch_idx + 1 == len(train_loader)):\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), getattr(config, 'max_grad_norm', 1.0))\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            try:\n",
        "                scheduler.step()  # Update learning rate\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        # timing measurement\n",
        "        if torch.cuda.is_available():\n",
        "            end_evt.record()\n",
        "            torch.cuda.synchronize()\n",
        "            batch_time = start_evt.elapsed_time(end_evt) / 1000.0   # Convert ms to seconds\n",
        "            peak_memory = torch.cuda.max_memory_allocated(device=config.device) / 1024**2   # MB\n",
        "        else:\n",
        "            batch_time = time.time() - start_time\n",
        "            peak_memory = 0.0\n",
        "\n",
        "        # Save stats\n",
        "        flash_stats['time_per_batch'].append(batch_time)\n",
        "        flash_stats['peak_memory_mb'].append(peak_memory)\n",
        "\n",
        "        # Update loss tracking\n",
        "        total_loss += loss.item() * grad_accum_steps\n",
        "        avg_loss = total_loss / (batch_idx + 1)\n",
        "\n",
        "        # log periodically\n",
        "        if (batch_idx % log_interval == 0) or (batch_idx + 1 == len(train_loader)):\n",
        "            try:\n",
        "                current_lr = scheduler.get_last_lr()[0]\n",
        "            except Exception:\n",
        "                current_lr = optimizer.param_groups[0]['lr']\n",
        "            pbar.set_postfix({\n",
        "                'loss': f\"{avg_loss:.4f}\",\n",
        "                'lr': f\"{current_lr:.3e}\",\n",
        "                'time(s)': f\"{batch_time:.4f}\",\n",
        "                'peak_mem(MB)': f\"{peak_memory:.1f}\"\n",
        "            })\n",
        "\n",
        "    # epoch cleanup\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.save(model.state_dict(), f\"gpt3_epoch{epoch+1}.pt\")\n",
        "    print(f\"Epoch {epoch+1} done. Avg loss: {avg_loss:.4f}, checkpoint saved.\")\n",
        "\n",
        "# --- final stats ---\n",
        "if flash_stats['time_per_batch']:\n",
        "    avg_time = sum(flash_stats['time_per_batch']) / len(flash_stats['time_per_batch'])\n",
        "    avg_mem = sum(flash_stats['peak_memory_mb']) / len(flash_stats['peak_memory_mb'])\n",
        "else:\n",
        "    avg_time, avg_mem = 0.0, 0.0\n",
        "\n",
        "print(f\"--- Training Summary ---\")\n",
        "print(f\"Average batch time: {avg_time:.6f} sec\")\n",
        "print(f\"Average peak memory: {avg_mem:.2f} MB\")\n",
        "print(\"Training complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXKDgIxaLYry"
      },
      "outputs": [],
      "source": [
        "#@title Visualization\n",
        "\n",
        "# --- Time per batch ---\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(flash_stats['time_per_batch'], label='Time per batch (s)')\n",
        "plt.xlabel('Batch')\n",
        "plt.ylabel('Seconds')\n",
        "plt.title('Batch Time per Batch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# --- Peak memory per batch ---\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(flash_stats['peak_memory_mb'], label='Peak memory (MB)', color='orange')\n",
        "plt.xlabel('Batch')\n",
        "plt.ylabel('Memory (MB)')\n",
        "plt.title('Peak GPU Memory Usage per Batch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# --- Average statistics ---\n",
        "avg_time = sum(flash_stats['time_per_batch']) / len(flash_stats['time_per_batch'])\n",
        "avg_mem = sum(flash_stats['peak_memory_mb']) / len(flash_stats['peak_memory_mb'])\n",
        "\n",
        "# Print overall average runtime and memory usage\n",
        "print(f\"Average time per batch: {avg_time:.6f} sec\")\n",
        "print(f\"Average peak memory usage: {avg_mem:.2f} MB\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}